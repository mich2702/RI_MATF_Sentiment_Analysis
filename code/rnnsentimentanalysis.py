# -*- coding: utf-8 -*-
"""RnnSentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AR9meNVkwwk1w5WYvUp5580F7202__jV
"""

!pip install classla

!pip install srtools

import pandas as pd
import re
from srtools import cyrillic_to_latin
import classla 
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer  
from tensorflow.keras.preprocessing.sequence import pad_sequences  
from tensorflow.keras.models import Sequential    
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN, Bidirectional, GRU
from tensorflow.keras.callbacks import ModelCheckpoint  
from tensorflow.keras.models import load_model   
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

"""**Load data**"""

df = pd.read_csv('/content/SentiComments.SR.corr.txt', sep = '\t', header = None, names = ['sentiment_label', 'comment_ID', 'comment_text'], index_col=False)
dfStop = pd.read_csv('/content/SerbianStopWords.txt', sep = '\n', header = None, index_col=False)
srbStopWord = list(dfStop[0])

def emoticons_pretprocessing(text):
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',
                           text)
    text = (re.sub('[\W]+', ' ', text.lower()) +
            ' '.join(emoticons).replace('-', ''))
    return text

def load_dataset():
  #translate from cyrillic to latin
  df['comment_text'] = df['comment_text'].map(lambda comment: cyrillic_to_latin(comment))

  x_data = df['comment_text']
  y_data = df['sentiment_label']

  #remove punctuation marks
  x_data = x_data.map(lambda comment: re.sub('[/.?!]','',comment))


  #making coexistent emoticons
  x_data = x_data.map(lambda comment: emoticons_pretprocessing(comment))  


  #removing 4-valent mapping
  y_data = y_data.map(lambda sent: re.sub('[/+].*','1',sent))
  y_data = y_data.map(lambda sent: re.sub('[/-].*','0',sent))
  y_data = y_data.map(lambda sent: int(sent))

  return x_data, y_data

"""**Lemmatisation**"""

classla.download('sr', type='nonstandard')
nlp = classla.Pipeline(lang = 'sr', type = 'nonstandard',processors='tokenize,pos,lemma')

def lemmatisation(text):
  doc = nlp(text)
  map = doc.to_dict()
  lemmes = [i['lemma'] for i in map[0][0]]
  return lemmes

"""**Tokenization**"""

x_data, y_data = load_dataset()
x_data = x_data.apply(lambda comment: lemmatisation(comment))

"""**Removing stop words**"""

# remove stop words
  x_data = x_data.map(lambda lemmas: [i for i in lemmas if i not in srbStopWord])

"""**Creating Training and Test data**"""

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)
# x_train = x_data[0:2768]
# x_test = x_data[2768:]

# y_train = y_data[0:2768]
# y_test = y_data[2768:]

x_train.shape

def get_max_length():
  comment_len = []
  for comment in x_train:
    comment_len.append(len(comment))
  return int(np.ceil(np.mean(comment_len)))

token = Tokenizer(lower=False) 
token.fit_on_texts(x_train)
x_train = token.texts_to_sequences(x_train)
x_test = token.texts_to_sequences(x_test)

max_length = get_max_length()

x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')
x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')

total_words = len(token.word_index) + 1 

print('Encoded X Train\n', x_train, '\n')
print('Encoded X Test\n', x_test, '\n')
print('Maximum review length: ', max_length)

EMBED_DIM = 32
LSTM_OUT = 64

# model = Sequential()
# model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))
# model.add(LSTM(LSTM_OUT))
# model.add(Dense(1, activation='sigmoid'))

# model = Sequential()
# model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))
# model.add(LSTM(units = 50, return_sequences = True, activation = 'relu'))
# model.add(Dropout(0.2))
# model.add(LSTM(units = 50, return_sequences = True, activation = 'relu'))
# model.add(Dropout(0.2))
# model.add(LSTM(units = 50,activation = 'relu'))
# model.add(Dropout(0.2))
# model.add(Dense(1, activation='sigmoid'))

# model = Sequential()
# model.add(Embedding(input_dim = total_words, output_dim = EMBED_DIM))
# model.add(SimpleRNN(EMBED_DIM, return_sequences = True))
# model.add(SimpleRNN(EMBED_DIM))
# model.add(Dense(1))

model = Sequential()
model.add(Embedding(total_words, EMBED_DIM))
model.add(Bidirectional(GRU(256)))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

#model = Sequential()
# model.add(Embedding(total_words, EMBED_DIM))
# model.add(Bidirectional(LSTM(LSTM_OUT)))
# model.add(Dense(64, activation = 'relu'))
# model.add(Dense(1, activation = 'sigmoid'))


model = Sequential()
model.add(Embedding(input_dim=total_words, output_dim=EMBED_DIM))
model.add(GRU(256, return_sequences=True))
model.add(SimpleRNN(128))
model.add(Dense(1, activation = 'sigmoid'))


model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

print(model.summary())

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")

model.fit(x_train, y_train, batch_size= 128, epochs = 10, callbacks = [tensorboard_callback], validation_split= 0.1)

model.evaluate(x_test, y_test, batch_size = 64)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

import matplotlib.pyplot as plt
import numpy as np

# Fixing random state for reproducibility
np.random.seed(19680801)

accuracy = [0.6460, 0.7240, 0.7178, 0.7298, 0.7355, 0.7442]

plt.rcdefaults()
fig, ax = plt.subplots()

# Example data
RNNS = ('FRNN', 'LSTM1', 'LSTM3', 'GRU', 'BLSTM','BGRU')
y_pos = np.arange(len(RNNS))
performance = 3 + 10 * np.random.rand(len(RNNS))

ax.barh(y_pos, accuracy, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(RNNS)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Accurancy')
ax.set_title('Accurancy of prediction')

plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Fixing random state for reproducibility
np.random.seed(19680801)

loss = [1.8539, 1.1483, 3.8157, 1.2952, 1.5156, 1.4928]

plt.rcdefaults()
fig, ax = plt.subplots()

# Example data
RNNS = ('FRNN', 'LSTM1', 'LSTM3', 'GRU','BLSTM','BGRU')
y_pos = np.arange(len(RNNS))
performance = 3 + 10 * np.random.rand(len(RNNS))

ax.barh(y_pos, loss, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(RNNS)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Loss')
ax.set_title('Loss of prediction')

plt.show()

